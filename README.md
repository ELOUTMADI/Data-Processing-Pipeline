# Real-Time Data Processing and Analysis Pipeline

You can find below the architecture that's been built : 

![Architecture](https://user-images.githubusercontent.com/47195793/198748185-2b761fb6-f96e-4d67-b2c5-5a86e1f9ed71.png)
## Introduction

This README file explains the details of a real-time data processing and analysis pipeline that has been built using the following technologies: 
- GCP Virtual Machine (VM)
- Apache Hadoop
- Apache Spark
- Apache Nifi
- Apache Cassandra
- MongoDB
- Python Dash

The pipeline ingests data in JSON format from a website (randomuser.me) and streams it to a Kafka broker. The data is then consumed and processed in a batch fashion and stored in Dataframe format in MongoDB and raw data in Cassandra. 

Apache Hadoop is used to store the necessary Jar files to run the code on Cloudera. The processed data is then used to display real-time data analysis on a dashboard implemented with Python Dash.

## Setup
- A VM has been created on GCP and SSH has been built using Putty for easy installation.
- CDH (Cloudera Distribution for Hadoop) has been installed on the VM to manage different technologies like Apache Hadoop, Spark, and Nifi.
- Data in JSON format is generated by connecting Apache Nifi to the website (randomuser.me) and new data is generated on each refresh and ingested to the Kafka Broker.
- The data is then consumed and processed in a batch fashion and stored in Dataframe format towards MongoDB and raw data to Cassandra.
- Apache Hadoop is used to store some Jar files to be able to run the code in Cloudera.


# Real-Time Data Pipeline

This code is a Real-Time Data Pipeline that **reads data** from a Kafka topic, processes it, and then stores it in a Cassandra and MongoDB cluster. 

## Connection Details
The code starts by defining the necessary connection details for the following: 
- Kafka broker/cluster
- Cassandra cluster
- MongoDB cluster

### Kafka Connection
- `kafka_topic_name`: The name of the topic that the pipeline will read data from.
- `kafka_bootstrap_servers`: The connection details of the Kafka broker/cluster.

### Cassandra Connection
- `cassandra_connection_host`: The hostname or IP address of the Cassandra cluster.
- `cassandra_connection_port`: The port number used to connect to the Cassandra cluster.
- `cassandra_keyspace_name`: The keyspace where the data will be stored in the Cassandra cluster.
- `cassandra_table_name`: The table name where the data will be stored in the Cassandra cluster.

### MongoDB Connection
- `mongodb_host_name`: The hostname or IP address of the MongoDB cluster.
- `mongodb_port_no`: The port number used to connect to the MongoDB cluster.
- `mongodb_user_name`: The username used to connect to the MongoDB cluster.
- `mongodb_password`: The password used to connect to the MongoDB cluster.
- `mongodb_database_name`: The database name where the data will be stored in the MongoDB cluster.

## Data Processing
The code then uses the SparkSession API to create a Spark session, which is used to **read data** from the Kafka topic and process it. The log level is set to "ERROR" to suppress any unnecessary logs.

The code then uses the `spark.readStream` method to **read data** from the Kafka topic specified in the `kafka_topic_name` variable. The `kafka_bootstrap_servers` variable is used to specify the Kafka broker/cluster details. The `transaction_detail_df` variable is used to store the data read from the Kafka topic.

After **reading** the data from the Kafka topic, the code uses the `from_json` method to convert the data from JSON format to a DataFrame and then it sets up the `transaction_detail_schema`, which is used to define the schema of the DataFrame. The schema includes various fields such as user details, transaction details, etc.

The code then performs some data transformation and filtering operations on the DataFrame. 

## Data Storage
After processing the data, the code writes the data to a Cassandra and MongoDB cluster using the `writeStream` method. The processed data is written to the specified keyspace and table in the Cassandra cluster, and the specified database in the MongoDB cluster.

## Execution
The code ends by starting the streaming process using the `start()` method. The pipeline will continue to run and process new data as it arrives on the specified Kafka topic, and it will store the processed data in the specified Cassandra and MongoDB clusters.

This pipeline can be useful for use cases where real-time data processing and storage are required, such as in e-commerce or retail applications where transaction data needs to be processed and stored in real-time for analytics and reporting.
